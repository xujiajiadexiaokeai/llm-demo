{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a786c77c",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "## Outline\n",
    "* ConversationBufferMemory\n",
    "* ConversationBufferWindowMemory\n",
    "* ConversationTokenBufferMemory\n",
    "* ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297dcd5",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f518f5",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "api_key = os.environ['STEP_API_KEY']\n",
    "base_url = os.environ['STEP_BASE_URL']\n",
    "llm_model = os.environ['STEP_LLM_MODEL']\n",
    "print(api_key)\n",
    "print(base_url)\n",
    "print(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301270b-7a69-40f8-9934-f840499b6ae9",
   "metadata": {},
   "source": [
    "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ad6fe2",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88bdf13d",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model, api_key=api_key, base_url=base_url)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db24677d",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Andrew\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello, Andrew! It's great to meet you. I'm an AI designed to assist and converse with users like you. I can help you with a wide range of topics, from answering questions to providing information on various subjects. Feel free to ask me anything you'd like to know!\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc3ef937",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello, Andrew! It's great to meet you. I'm an AI designed to assist and converse with users like you. I can help you with a wide range of topics, from answering questions to providing information on various subjects. Feel free to ask me anything you'd like to know!\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"1+1 equals 2. This is a basic arithmetic operation that combines two numbers, in this case, 1 and 1, to produce a sum. The result is 2, which is the smallest positive even number. It's also the first number in the Fibonacci sequence, a series of numbers where each term is the sum of the two preceding ones.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf3339a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello, Andrew! It's great to meet you. I'm an AI designed to assist and converse with users like you. I can help you with a wide range of topics, from answering questions to providing information on various subjects. Feel free to ask me anything you'd like to know!\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 equals 2. This is a basic arithmetic operation that combines two numbers, in this case, 1 and 1, to produce a sum. The result is 2, which is the smallest positive even number. It's also the first number in the Fibonacci sequence, a series of numbers where each term is the sum of the two preceding ones.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your name is Andrew. You introduced yourself to me when you first started our conversation. It's always nice to know the name of the person I'm talking to, as it helps personalize our interaction. If you have any other questions or need assistance with something, feel free to ask!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2529400d",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Andrew\n",
      "AI: Hello, Andrew! It's great to meet you. I'm an AI designed to assist and converse with users like you. I can help you with a wide range of topics, from answering questions to providing information on various subjects. Feel free to ask me anything you'd like to know!\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 equals 2. This is a basic arithmetic operation that combines two numbers, in this case, 1 and 1, to produce a sum. The result is 2, which is the smallest positive even number. It's also the first number in the Fibonacci sequence, a series of numbers where each term is the sum of the two preceding ones.\n",
      "Human: What is my name?\n",
      "AI: Your name is Andrew. You introduced yourself to me when you first started our conversation. It's always nice to know the name of the person I'm talking to, as it helps personalize our interaction. If you have any other questions or need assistance with something, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5018cb0a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Andrew\\nAI: Hello, Andrew! It's great to meet you. I'm an AI designed to assist and converse with users like you. I can help you with a wide range of topics, from answering questions to providing information on various subjects. Feel free to ask me anything you'd like to know!\\nHuman: What is 1+1?\\nAI: 1+1 equals 2. This is a basic arithmetic operation that combines two numbers, in this case, 1 and 1, to produce a sum. The result is 2, which is the smallest positive even number. It's also the first number in the Fibonacci sequence, a series of numbers where each term is the sum of the two preceding ones.\\nHuman: What is my name?\\nAI: Your name is Andrew. You introduced yourself to me when you first started our conversation. It's always nice to know the name of the person I'm talking to, as it helps personalize our interaction. If you have any other questions or need assistance with something, feel free to ask!\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14219b70",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a36e9905",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61631b1f",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2fdf9ec",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ca79256",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "890a4497",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98e9ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66eeccc3",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ea6233e",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc4553fb",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a788403",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4087bc87",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model, api_key=api_key, base_url=base_url)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4faaa952",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, Andrew! It's great to meet you. I'm an AI designed to assist and converse with users like you. I can help you with a wide range of topics, from answering questions to providing information on various subjects. Feel free to ask me anything you'd like to know!\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb20ddaa",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1+1 equals 2. This is a basic arithmetic operation that combines two numbers, in this case, 1 and 1, to produce a sum. The result is 2, which is the smallest positive even number. It's also the first number in the Fibonacci sequence, a series of numbers where each term is the sum of the two preceding ones.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "489b2194",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't know your name. As an AI, I don't have access to personal information unless you provide it to me during our conversation. If you'd like to tell me your name, I'd be happy to remember it for future interactions.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2931b92",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f6d063c",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.11/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
      "Downloading tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl (907 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m907.0/907.0 kB\u001b[0m \u001b[31m121.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb9020ed",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model, api_key=api_key, base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43582ee6",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model cl100k_base.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationTokenBufferMemory(llm\u001b[38;5;241m=\u001b[39mllm, max_token_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI is what?!\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      3\u001b[0m                     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmazing!\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      4\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackpropagation is what?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      5\u001b[0m                     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeautiful!\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      6\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbots are what?\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \n\u001b[1;32m      7\u001b[0m                     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCharming!\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/memory/token_buffer.py:54\u001b[0m, in \u001b[0;36mConversationTokenBufferMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Prune buffer if it exceeds max token limit\u001b[39;00m\n\u001b[1;32m     53\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39mmessages\n\u001b[0;32m---> 54\u001b[0m curr_buffer_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mget_num_tokens_from_messages(buffer)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token_limit:\n\u001b[1;32m     56\u001b[0m     pruned_memory \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:629\u001b[0m, in \u001b[0;36mChatOpenAI.get_num_tokens_from_messages\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m    627\u001b[0m     tokens_per_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/openai/openai-python/blob/main/chatml.md for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformation on how messages are converted to tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    634\u001b[0m     )\n\u001b[1;32m    635\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    636\u001b[0m messages_dict \u001b[38;5;241m=\u001b[39m [convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model cl100k_base.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284288e1",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff55d5d",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72dcf8b1",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a5b238f",
   "metadata": {
    "height": 268,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model cl100k_base.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m schedule \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is a meeting at 8am with your product team. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mYou will need your powerpoint presentation prepared. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m9am-12pm have time to work on your LangChain \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mfrom over an hour away to meet you to understand the latest in AI. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124mBe sure to bring your laptop to show the latest LLM demo.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationSummaryBufferMemory(llm\u001b[38;5;241m=\u001b[39mllm, max_token_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello\u001b[39m\u001b[38;5;124m\"\u001b[39m}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms up\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     12\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot much, just hanging\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     13\u001b[0m                     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCool\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     14\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is on the schedule today?\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \n\u001b[1;32m     15\u001b[0m                     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschedule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/memory/summary_buffer.py:61\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave_context(inputs, outputs)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain/memory/summary_buffer.py:66\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.prune\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39mmessages\n\u001b[0;32m---> 66\u001b[0m curr_buffer_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mget_num_tokens_from_messages(buffer)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token_limit:\n\u001b[1;32m     68\u001b[0m     pruned_memory \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:629\u001b[0m, in \u001b[0;36mChatOpenAI.get_num_tokens_from_messages\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m    627\u001b[0m     tokens_per_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/openai/openai-python/blob/main/chatml.md for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformation on how messages are converted to tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    634\u001b[0m     )\n\u001b[1;32m    635\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    636\u001b[0m messages_dict \u001b[38;5;241m=\u001b[39m [convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model cl100k_base.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ecabe",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728edba",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a221b1d",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb582617",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6709b89-182e-4b7b-b811-1584dda20652",
   "metadata": {},
   "source": [
    "Reminder: Download your notebook to you local computer to save your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba827aa",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07922b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52696c8c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48690d13",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bba1f8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a62e89",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953ad8d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb2617",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1989bf",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8dca6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9120949",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55849fb5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae491b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3aec3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae51a3f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06894001",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb674c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158f4c8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cebe066",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65caee4d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04799230",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30795c10",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed59c4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb43eee3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
